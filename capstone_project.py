# -*- coding: utf-8 -*-
"""Capstone project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cuKFxPp2Gahi4gwX0xJSU_BGSJe0RZD2
"""

# Installing necessary dependencies
try:
    import pyspark
except ModuleNotFoundError:
    !pip install pyspark

try:
    import tensorflow as tf
except ModuleNotFoundError:
    !pip install tensorflow

try:
    from imblearn.over_sampling import SMOTE
except ModuleNotFoundError:
    !pip install imbalanced-learn

try:
    from flask import Flask, request, jsonify
except ModuleNotFoundError:
    !pip install flask

!pip install keras
!pip install scikit-learn

"""# New Section"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras import backend as K
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Capstone Project") \
    .getOrCreate()

# Custom R-squared metric
def r2_keras(y_true, y_pred):
    ss_res = K.sum(K.square(y_true - y_pred))
    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))
    return 1 - ss_res / (ss_tot + K.epsilon())

# Data processing class
class DataProcessor:
    def __init__(self, data_path):
        self.data_path = data_path
        self.df = self._load_data()
        self.scalers = { 'Close': StandardScaler(), 'Open': StandardScaler(), 'Volume': StandardScaler() }
        self.feature_scalers = { 'Close': StandardScaler(), 'Open': StandardScaler(), 'Volume': StandardScaler() }

    def _load_data(self):
        df = spark.read.csv(self.data_path, header=True, inferSchema=True, sep=",", multiLine=True)
        df = df.filter(df['Date'].isNotNull())
        df = df.select('Date', 'Close', 'Open', 'Volume')
        return df

    def etl_process(self):
        self.df = self.df.dropna()

    def feature_creation(self):
        pd_df = self.df.toPandas()
        pd_df['Date'] = pd.to_datetime(pd_df['Date'], format='%Y-%m-%d')

        for col in ['Close', 'Open', 'Volume']:
            pd_df[f'{col}_scaled'] = self.scalers[col].fit_transform(pd_df[[col]])

        # Create lag features
        for col in ['Close', 'Open', 'Volume']:
            for lag in range(1, 8):
                pd_df[f'{col}_lag_{lag}'] = pd_df[f'{col}_scaled'].shift(lag)

        pd_df.dropna(inplace=True)

        # Scale lag features
        for col in ['Close', 'Open', 'Volume']:
            numerical_cols = [f'{col}_lag_{lag}' for lag in range(1, 8)]
            pd_df[numerical_cols] = self.feature_scalers[col].fit_transform(pd_df[numerical_cols])

        self.processed_data = pd_df

    def get_feature_target(self, target):
        feature_columns = [col for col in self.processed_data.columns if col.startswith(f'{target}_lag')]
        X = self.processed_data[feature_columns]
        y = self.processed_data[f'{target}_scaled']
        return X, y

    def save_feature_columns(self, feature_columns, target):
        np.save(f'{target}_feature_columns.npy', feature_columns)

    def load_feature_columns(self, target):
        return np.load(f'{target}_feature_columns.npy', allow_pickle=True)

    def save_scaler(self, scaler, target):
        np.save(f'{target}_scaler.npy', scaler)

    def load_scaler(self, target):
        return np.load(f'{target}_scaler.npy', allow_pickle=True).item()

# Model handling class
class ModelHandler:
    def __init__(self, input_shape):
        self.model = self._define_model(input_shape)

    def _define_model(self, input_shape):
        model = Sequential()
        model.add(Dense(64, input_shape=(input_shape,), activation='relu'))
        model.add(Dropout(0.5))
        model.add(Dense(32, activation='relu'))
        model.add(Dense(1))
        model.compile(optimizer=RMSprop(), loss='mean_squared_error', metrics=['mean_squared_error', r2_keras])
        return model

    def train_model(self, X_train, y_train, X_val, y_val, epochs=20, batch_size=32):
        history = self.model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)
        return history

    def evaluate_model(self, X_test, y_test):
        predictions = self.model.predict(X_test)
        mse = mean_squared_error(y_test, predictions)
        mae = mean_absolute_error(y_test, predictions)
        r2 = r2_score(y_test, predictions)
        print(f"Mean Squared Error: {mse}")
        print(f"Mean Absolute Error: {mae}")
        print(f"R-squared: {r2}")
        return mse, mae, r2

    def save_model(self, model_path="model.h5"):
        self.model.save(model_path)
        print(f"Model saved as {model_path}")

    def predict_next_month(self, last_month_data, feature_scaler, feature_columns, value_scaler):
        predictions = []
        for _ in range(30):
            last_month_df = pd.DataFrame(last_month_data, columns=feature_columns)
            scaled_data = feature_scaler.transform(last_month_df)
            next_prediction = self.model.predict(scaled_data)
            next_prediction_descaled = value_scaler.inverse_transform(next_prediction)
            predictions.append(next_prediction_descaled[0, 0])
            last_month_data = np.roll(last_month_data, -1)
            last_month_data[-1] = next_prediction[0, 0]
        return predictions

# Plotting utility class
class Plotter:
    @staticmethod
    def plot_data(processed_data, next_month_predictions, next_month_dates, target):
        plt.figure(figsize=(15, 10))
        plt.subplot(2, 2, 1)
        plt.plot(processed_data['Date'], processed_data[target], label='Original Data')
        plt.title(f'All Original Data ({target})')
        plt.xlabel('Date')
        plt.ylabel(target)
        plt.legend()

        plt.subplot(2, 2, 2)
        plt.plot(processed_data['Date'].tail(30), processed_data[target].tail(30), label='Last Month')
        plt.title(f'Last Month Data ({target})')
        plt.xlabel('Date')
        plt.ylabel(target)
        plt.legend()

        plt.subplot(2, 2, 3)
        plt.plot(next_month_dates, next_month_predictions, label='Predictions', color='orange')
        plt.title(f'Next Month Predictions ({target})')
        plt.xlabel('Date')
        plt.ylabel(target)
        plt.legend()

        plt.subplot(2, 2, 4)
        plt.plot(processed_data['Date'], processed_data[target], label='Original Data')
        plt.plot(next_month_dates, next_month_predictions, label='Predictions', color='orange')
        plt.title(f'All Data Including Predictions ({target})')
        plt.xlabel('Date')
        plt.ylabel(target)
        plt.legend()

        plt.tight_layout()
        plt.show()

    @staticmethod
    def plot_scatter_data(processed_data, next_month_predictions, next_month_dates, target):
        plt.figure(figsize=(10, 6))
        plt.scatter(processed_data['Date'], processed_data[target], label='Original Data', alpha=0.6)
        plt.scatter(next_month_dates, next_month_predictions, label='Predictions', color='orange', alpha=0.6)
        plt.title(f'All Data Including Predictions ({target}) (Scatter Plot)')
        plt.xlabel('Date')
        plt.ylabel(target)
        plt.legend()
        plt.show()

# Main pipeline class
class Pipeline:
    def __init__(self, data_path):
        self.data_processor = DataProcessor(data_path)
        self.data_processor.etl_process()
        self.data_processor.feature_creation()

    def run(self):
        targets = ['Close', 'Open', 'Volume']
        for target in targets:
            X, y = self.data_processor.get_feature_target(target)
            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

            input_shape = X_train.shape[1]
            model_handler = ModelHandler(input_shape)
            model_handler.train_model(X_train, y_train, X_val, y_val)
            model_handler.evaluate_model(X_val, y_val)

            feature_columns = X.columns
            self.data_processor.save_feature_columns(feature_columns, target)
            self.data_processor.save_scaler(self.data_processor.feature_scalers[target], target)

            model_handler.save_model(f"model_{target.lower()}.keras")

            last_month_data = X.tail(30).values
            if last_month_data.shape[0] != 30:
                last_month_data = np.tile(last_month_data, (30, 1))

            feature_columns = self.data_processor.load_feature_columns(target)
            feature_scaler = self.data_processor.load_scaler(target)
            next_month_predictions = model_handler.predict_next_month(last_month_data, feature_scaler, feature_columns, self.data_processor.scalers[target])
            next_month_dates = pd.date_range(start=self.data_processor.processed_data['Date'].max() + pd.Timedelta(days=1), periods=30)

            Plotter.plot_data(self.data_processor.processed_data, next_month_predictions, next_month_dates, target)
            Plotter.plot_scatter_data(self.data_processor.processed_data, next_month_predictions, next_month_dates, target)

if __name__ == "__main__":
    data_path = "/content/Meta Dataset.csv"  # Change the path to the uploaded file
    pipeline = Pipeline(data_path)
    pipeline.run()